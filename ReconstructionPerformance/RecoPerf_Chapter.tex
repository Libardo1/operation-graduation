% Activate the following line by filling in the right side. If for example the name of the root file is Main.tex, write
% "...root = Main.tex" if the chapter file is in the same directory, and "...root = ../Main.tex" if the chapter is in a subdirectory.
 
%!TEX root =  ../Thesis.tex

\chapter[Reconstruction and Performance]{Event Reconstruction and Performance}
Once the physics data has been created by the LHC, detected by the ATLAS detector, and written to 
disk by the ATLAS trigger and data acquisition system, it is reconstructed into physics events.  This process has 
many interacting steps.  First, hits in the inner detector are algorithmically combined into tracks, which approximate the 
trajectory of charged particles, while energy clusters in the calorimeters are grouped together into jets, which seek to 
capture the energy of the particle that originated the jet.  

Higher-order quantities and corrections also come into play.  Two of the most important are the calibration and removal of 
pileup, other lower-energy interactions from the same or adjacent bunch crossings, and $b$-tagging
, which primarily uses tracks to identify jets that are likely originated by $b$-quarks.  Additionally, 
there are jet energy corrections that account for jet radiation that could by lost by the jet clustering algorithms, 
$b$-tagging efficiency corrections, and uncertainties on the total luminosity collected. 



\section{Track Reconstruction}
\label{sec:trk_reco}
%https://twiki.cern.ch/twiki/pub/AtlasPublic/EventDisplayStandAlone/2012_highPileup.png
Track reconstruction takes place in the inner detector and allows crucial measurements such as the primary and secondary vertices.  
When a charged particle traverses the layers of the inner detector, it leaves typically 8-11 hits in 
silicon (counting one double-sided layer of SCT silicon as capable of seeing 2 hits) and about 
35 in the TRT.  The track reconstruction algorithm  
starts by identifying track seeds and then iterating through a Kalman filter algorithm, projecting out to further tracking layers 
and then checking for hits along the hypothesized trajectory of the particle \cite{newt}.  

Track reconstruction performance can be impacted by pileup, as more pileup (and hence more tracking hits) creates 
more opportunities for incorrectly assigned hits and fake tracks from random hits being falsely associated with each other.  A 
second effect of pileup on tracking is indirect: the detector as a whole, and the silicon detectors in 
particular, undergo radiation damage as luminosity accrues.  Radiation damage in silicon detectors manifests 
itself as rising leakage current, which makes channels prone to more noise hits unless the thresholds are raised (
but higher thresholds lowers the efficiency for legitimate hits).  As time goes on, the tracking quality can can 
be affected by the extra noise hits and/or higher thresholds.  The track reconstruction efficiency and resolution
can be seen in Figures~\ref{fig:track_perfA} and ~\ref{fig:track_perfB}.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{ReconstructionPerformance/images/track_perf1.pdf}
	\includegraphics[width=0.5\textwidth]{ReconstructionPerformance/images/track_perf2.pdf}
	\label{fig:track_perfA}  
	\caption{The efficiency of the track reconstruction, projected in $p_T$ and $\eta$, as computed in Monte Carlo.}
\end{figure}



\begin{figure}
	\includegraphics[width=0.5\textwidth]{ReconstructionPerformance/images/track_perf3.pdf}
	\includegraphics[width=0.5\textwidth]{ReconstructionPerformance/images/track_perf4.pdf}
	\label{fig:track_perfB}  
	\caption{The track resolution in mm the $x$ and $z$ directions, comparing minimum bias simulation to data taken with a random trigger.}
\end{figure}

Good tracking performance enters this analysis most directly in its impact on the tagging of $b$-jets, as detailed in Section~\ref{sec:b-tag}.  Well-reconstructed tracks allow for the identification of a secondary vertex, which is one of the fundamental features of the ATLAS $b$-jet tagging algorithms.

% http://www.slac.stanford.edu/cgi-wrap/getdoc/slac-pub-5671.pdf
% http://www.quantumdiaries.org/2011/06/10/to-b-or-not-to-bbar-b-tagging-via-track-counting/
% http://dorigo.wordpress.com/2007/05/15/b-hadron-lifetimes-part-2/

Measurements from the inner detector (pixel, SCT and TRT) are combined in the track reconstruction.  There is a broad range 
of properties that tracks might have, so the inner detector has to be able to measure tracks with p$_T$
 ranging from 150 MeV to 30 GeV or more.   

%The track reconstruction Kalman filter algorithm operates in a few steps.  It begins with track seeds, which are collections of a few hits in the pixel subsystem that could plausibly be 
%parts of tracks.  The advantage of starting with seeds, rather than attempting to find an entire track at 
%once, is that seeds allow flexibility at the beginning of the search (when the track trajectory is the 
%least known) while keeping computational costs down.  Once a seed has been identified, the Kalman fitter (
%as the ATLAS algorithm is called) projects where the next hit would be if there is truly a track 
%present, and then looks for the presence of a hit at the predicted location in the next tracker layer
%.  If a hit is found there, the predicted trajectory of the particle may be refined and the next 
%hit is predicted and sought, until all the detector layers have been traversed. If there is no hit 
%present where one is predicted, the Kalman fitter can project two layers further, to allow for the possibility 
%of a hole where the particle did not leave a hit for some reason. 
%
%In high-pileup environments, the inner detector occupancy can be high which can make for two tracks that 
%overlap, leaving a hit in the same place which must then be allocated to one or the other.  
%Dedicated ambiguity solving is implemented as a second stage in the track reconstruction, where the two candidate tracks are 
%scored against each other via a reward/penalty scheme.  A good $\chi^2/DOF$ 
%score is rewarded, a track with many holes on track or a low $\chi^2/DOF$ 
%is penalized, with subdetector-specific weighting favoring the higher-precision silicon over the the lower-precision TRT \cite{newt}. 



%http://iopscience.iop.org/1126-6708/2008/04/063/pdf/1126-6708_2008_04_063.pdf
\section{Jet Reconstruction}
\label{sec:jet_reco}
Because of asymptotic freedom of QCD, quarks and gluons do not remain standalone particles once they're produced, 
but instead they hadronize and shower as they travel through the detector.  Hypothetically, all the particles of 
the shower can be added back up to approximate the energy, momentum, and angle of the original quark 
or gluon.  Jet reconstruction is the process of assembling calorimeter deposits together into a physics object, called a 
jet, that ideally will do a good job of representing the characteristics ($p_T$, energy, 
flavor) of the quark or gluon that originated the jet.  There are a number of clustering algorithms for 
assembling the calorimeter cells, and post-processing steps for improving the performance of jets in analyses--pileup 
subtraction, energy calibrations, grooming, and trimming, to name a few.  

The default jet clustering algorithm in ATLAS is the anti-$k_t$ algorithm \cite{antikt}
with a distance parameter of 0.4.  Roughly summarized, this algorithm starts with a calorimeter cell that 
has an energy deposit at least $4\sigma$ higher in energy than the ambient and pileup noise, 
and then the surrounding cells with at least $2\sigma$ more energy than noise are grouped into 
the jet in a way that prioritizes high energy over close proximity.  The result is that soft deposits get 
clustered in with hard deposits, rather than clustering amongst themselves.  The distance parameter of 0.4 is 
a cutoff as to how far away from the seed in $\eta-\phi$ space to look for 
additional deposits.   Jets below 20 GeV or so can be difficult to distinguish from noise, so in practice, a lower limit on 
the $p_T$ (in the case of this analysis, 25 GeV) is often applied.  A visualization of the jet reconstruction that results
from the anti-$k_t$ algorithm can be seen in Figure~\ref{fig:anti_kt_jets}.

%A critical feature of this algorithm, or any jet algorithm, is that it be infrared and collinear safe
%.  In technical terms, that means that when additional `ghost' particles with infinitesimally small energy or infinitesimally 
%close radius are added to the area in and around the jet, the properties of the resulting jet do 
%not change.  In practical terms, it means that the jet is stable to small perturbations, so that 
%the presence or absence of low-energy nearby radiation (which can be difficult or impossible to distinguish from 
%calorimeter noise) or the precise direction of energy that is highly collinear with the jet itself (the direction 
%of this energy can be prone to resolution uncertainties) do not affect the jet properties.  Critically, the 
%anti-$k_t$ algorithm is both infrared and collinear safe.

\begin{figure}
    \center
	\includegraphics[width=0.6\textwidth]{ReconstructionPerformance/images/anti_kt_jets.png}
	\caption{A graphical representation of the position and energy of calorimeter deposits, and their clustering and reconstruction into jets by the anti-$k_t$ jet clustering algorithm.  A feature of anti-$k_t$ which is immediately apparent is the circular jet shapes that it tends to make \cite{antikt}.  \label{fig:anti_kt_jets}  }
\end{figure}







%https://twiki.cern.ch/twiki/bin/view/AtlasPublic/LuminosityPublicResults#Data_Taking_Efficiency_and_Pileu

\section{Pileup Calibration and Removal}
\label{sec:pileup}
All the detector subsystems are affected by the presence of pileup, which are collisions other than the hard scatter 
collision.  As the LHC delivers higher luminosity for a given number of proton bunches, the luminosity increase comes 
at the price of many interactions per bunch crossing, and these softer interactions create extra activity in the detector 
that tends to make events noisier and more challenging to reconstruct accurately.  In 2012, the mean number of 
interactions per crossing ranged from about 10 up to about 40.  

The inner detector and tracking provide an important tool for understanding in-time pileup.  In-time pileup 
is additional soft interactions in the same bunch crossing as the hard scatter.  The tracking allows primary vertex reconstruction 
with a resolution fine enough in z$_0$, for the pixels typically $z_0\sin\theta$, 
to resolve separate primary vertices from each other \cite{pileup_tracks}.   The calorimeters cannot resolve individual primary vertices with such precision, 
though, so a constant struggle in ATLAS is to measure the calorimeter deposits that come from pileup interactions, 
and where possible to apply corrections that subtract away pileup contributions to jets from the hard scatter.  On average
, each additional pileup vertex in an event adds 370(850) \cite{pileup} MeV to the p$_T$ of a jet reconstructed with the anti-$k_t$ algorithm with R=0.4(0.6).

In addition to in-time pileup, the calorimeters are prone to out-of-time pileup where 
the signal in a given in event can be affected by the energy flow of previous collisions because of the 
calorimeter readout signal shapes.  Out-of-time pileup has the effect of adding an average of 60
(210) MeV to central jets, and decreasing the forward jets by 350(470) MeV.  


\begin{figure}
    \center
	\includegraphics[width=0.6\textwidth]{ReconstructionPerformance/images/mu_2012-dec.pdf}
	\caption{A distribution of the mean number of interactions per crossing, also called $\mu$ seen in 2012 data-taking.  Larger $\mu$ values are characterized by more challenging reconstruction. 	\label{fig:mu_2012}  }
\end{figure}

% http://cds.cern.ch/record/1435196/files/ATLAS-CONF-2012-042.pdf
%http://iopscience.iop.org/1742-6596/119/3/032033/pdf/1742-6596_119_3_032033.pdf
\section{Primary Vertex Identification}
\label{sec:pv}
Both the hard scatter collision and pileup collisions produce charged tracks; once these tracks are reconstructed, they can 
be traced back to the interaction region of the ATLAS detector and used to figure out where collisions took place
.  Each time a group of tracks can be clustered together into a presumed proton-proton collision, we refer to it 
as a primary vertex.  The reconstruction of primary vertices is one of the reasons that tracking resolution must be 
so precise; primary vertices are often separated by only a few mm so imprecisely measured tracks can cause unintended 
merging.  The consequence of mistakes in the primary vertex reconstruction can be physics objects that get grouped with the 
wrong primary vertex, and lead to incorrect event reconstruction and either signal loss or undesired background acceptance.

The reconstruction errors on the primary vertex are generally within 0.02 mm in the x/y directions 
(transverse to the beam direction) and 0.1 mm in the z direction (longitudinal to the  
beam) \cite{pileup_tracks}.   This resolution can be seen in Figure~\ref{fig:pileup_pv}, a 2012 
$Z\rightarrow\mu\mu$ event with 25 reconstructed primary vertices.



%-----------------------------------------------------------------------------
\begin{figure}
    \center
	\includegraphics[width=0.5\textwidth]{ReconstructionPerformance/images/2012_highPileup.pdf}
	\caption{A data event taken in 2012, showing the occupancy typical of high pileup.  This particular event contains a $Z\rightarrow \mu\mu$ physics signature, mixed in with 25 primary vertices.	\label{fig:pileup_pv}  }
\end{figure}
%-----------------------------------------------------------------------------


\section{$b$-Tagging}
\label{sec:b-tag}

Although the LHC produces huge numbers of hadronic jets, those jets that arise from $b$-quarks
(which are called, appropriately, $b$-jets) are particularly important and interesting,
especially for this thesis where all the final state particles are $b$-quarks and so the 
experimental signature consists entirely of $b$-jets.
Once they are produced, $b$-quarks hadronize nearly instantaneously, but then
the $b$-hadrons have lifetimes that are typically on the order of 1ps before
decaying to other particles. 
Since $b$-quarks are often created in high-p$_T$ collisions or come from the decay 
of heavy particles, the resulting $b$-hadrons and $b$-mesons can have considerable $p_T$ and travel a few millimeters to a few centimeters before decaying.
When they do decay, there is often a reconstructible secondary vertex that is offset from
the primary vertex and can be used to identify, or tag, the jet as coming from a $b$-quark.  
%There are several $b$-tagging algorithms that look for particular characteristics of $b$-jets:
ATLAS offers $b$-tagging at both the trigger level and offline; this analysis makes use of both.


%-----------------------------------------------------------------------------
\begin{figure}
    \center
	\includegraphics[width=0.6\textwidth]{ReconstructionPerformance/images/secondary_vertex.pdf}
	\caption{An event from early ATLAS data-taking, with a number of tracks (in teal) leading back to the primary vertex, and a secondary vertex reconstructed with the associated tracks highlighted in purple \cite{sv}.	\label{fig:secondary_vertex}  }
\end{figure}
%-----------------------------------------------------------------------------



\subsection{Online $b$-Tagging}
The first opportunity for $b$-tagging occurs in the trigger.  The challenge of trigger (also called online)
$b$-tagging is that it requires secondary vertex identification, which in turn requires track reconstruction,
which requires the full readout of the inner detector and is also computationally expensive.  The upside,
however, is that $b$-jets occur at a much lower rate than light flavor jets in background, and are an important
part of many search signatures, so being able to identify $b$-jets in the trigger allows for lower
thresholds than could otherwise be supported.  The trigger in this analysis uses online $b$-tagging to keep
thresholds low. 

Like its offline counterpart, the online $b$-tagging algorithm has three major steps \cite{online_btag_2}:
\begin{itemize}
    \item reconstructing the tracks of charged particles
    \item identifying the primary vertex
    \item calculating a discriminant weight (in 2012, a likelihood weight) based on the signed impact parameter significance 
    (Figure ~\ref{fig:ip_sig}) for the reconstructed tracks and the properties of secondary vertices
\end{itemize}

These steps are repeated both at L2 and EF; so there are two independent layers of $b$-tagging applied by the
trigger.

%https://twiki.cern.ch/twiki/bin/view/AtlasPublic/BJetTriggerPublicResults
\begin{figure}
    \center
   \includegraphics[width=0.45\textwidth]{ReconstructionPerformance/online_btag_IP.pdf}
   \caption{The online signed impact parameter significance, as used in the trigger $b$-tagging algorithms. \label{fig:ip_sig} }
\end{figure}


\begin{figure}
	\includegraphics[width=0.45\textwidth]{ReconstructionPerformance/online_btag_L2.pdf}
	\includegraphics[width=0.45\textwidth]{ReconstructionPerformance/online_btag_EF.pdf}
	\caption{The jet weight distribution for jets undergoing online $b$-tagging at L2 (left) and EF (right),
        with a comparison between MC predictions of various truth flavors (colored histograms) and data (points).
        As the $b$-tagging weight increases, the relative enrichment of $b$-jets grows, although the overall
        efficiency for $b$-jets goes down as tighter online weights are applied. \cite{online_btag}	\label{fig:online_btag}  }
\end{figure}




\subsection{Offline $b$-Tagging}
Offline $b$-tagging is very similar to online tagging, in that it uses reconstructed tracks to gather
information about whether a secondary vertex might be present.  There are several specialized algorithms
that are each designed to exploit a specific feature of decays associated with $b$-jets:

\begin{itemize}
	\item large significance of the impact parameter, $d_0$ (IP3D \cite{ip3d})
	\item large ratio between the sum of $p_T$ of all tracks associated with the secondary vertex to the sum of all tracks in the jet (SV1 \cite{ip3d})
	\item large secondary vertex mass (SV1 \cite{ip3d})
	\item evidence of a decay chain including both a secondary vertex (from the $b$ decay) and a tertiary vertex (from the $c$ quark daughter of the $b$ quark) along the same line of flight (JetFitter \cite{jet_fitter}) 
\end{itemize}

Each of these features/algorithms can independently give discriminating power for tagging $b$-jets.
Additionally, though, if a jet truly comes from a $b$-quark, then several of these features can arise in the 
same jet and the correlation can be used to further increase the accuracy of the tagger.  That idea gives 
rise to the MV1 tagger (where the MV stands for ``multivariate''), a neural-net-based 
$b$-tagging algorithm that uses three other $b$-tagging algorithms (SV1, IP3D, JetFitter
) as inputs.   The performance curve for the MV1 algorithm can be seen in Figure~\ref{fig:mv1_roc}; 
for a typical $b$-jet efficiency of 70\% (meaning that 70\% of real $b$-jets 
are positively tagged by the algorithm), the light-jet rejection is about 99.5\% and the 
charm rejection is about 90\% \cite{b-tagging}.


%https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/CONFNOTES/ATLAS-CONF-2014-046/fig_01.pdf
\begin{figure}
    \center
	\includegraphics[width=0.8\textwidth]{ReconstructionPerformance/images/mv1_roc.pdf}
	\caption{The receiver-operator curve (ROC curve) for the MV1 algorithm, showing the tradeoff
    between light-flavor rejection (vertical axis) and $b$-jet efficiency (horizontal axis)
     \cite{b-tagging}.	\label{fig:mv1_roc}  }
\end{figure}


The importance of event reconstruction cannot be overstated.  Simply 
put, it is the reconstruction step that takes a set of raw detector 
measurements and turns them into a comprehensive description of the 
physics going on in the event--the energy of the hadronic and electromagnetic
particles, the trajectories of charged particles, the identification of 
$b$-jets and other types of particle identification, and so on.  However,
in order to truly understand the signal and background processes, it usually
takes studies on a deeper level than what we can get with purely data-driven methods;
we need a ``God's eye'' view of the collision and outgoing particles.  
We get this view by simulating physics events for ourselves using Monte Carlo
methods, and using those simulated events to devise the analysis strategy.
The next chapter will explain the details of that process.












